# ğŸš€ WorldCat Metadata Fetcher - Final Clean Version
# -----------------------------------------------
# âœ… Uses only JSON Discovery API (no XML, no unused OAuth)
# âœ… Reads OCLC numbers from hardcoded list
# âœ… Extracts title, creator, contributor, publisher, date, language, subjects, type, format, ISBN, ISSN, edition
# âœ… Writes to UTF-8 CSV

import os
import unicodedata
import re
import requests
import base64
import xml.etree.ElementTree as ET
import csv
from tqdm import tqdm
from html import unescape
import time
import json

# === USER CONFIGURATION ===
WS_KEY = "YOUR_WS_KEY"
WS_SECRET = "YOUR_WS_SECRET"
OCLC_NUMBERS = [
    "976446533",
    "1416597355",
    "1090108276"
]  # Add your OCLC numbers here

# ğŸ”¹ OUTPUT FILE PATH
OUTPUT_CSV_PATH = os.path.join(os.path.expanduser("~"), "Downloads", "worldcat_metadata_output.csv")

def normalize_unicode(text):
    """Normalize Unicode characters to their canonical form."""
    if not text:
        return ""
    # Normalize to NFC (canonical decomposition, then canonical composition)
    return unicodedata.normalize('NFC', str(text))

def fix_character_encoding(text):
    return text  # No transformation â€” preserve full Unicode
    """Enhanced character encoding fix with comprehensive Unicode handling."""
    if not text:
        return ""
    
    if not isinstance(text, str):
        text = str(text)
    
    # First, unescape HTML entities
    text = unescape(text)
    
    # Remove control characters except newlines, tabs
    text = ''.join(char for char in text if ord(char) >= 32 or char in '\n\r\t')
    
    # Fix common MARC21 encoding issues
    marc_fixes = {
        # Combining diacritics (common in MARC records)
        '\u0301': '',  # combining acute accent (handle separately)
        '\u0300': '',  # combining grave accent
        '\u0303': '',  # combining tilde
        '\u0308': '',  # combining diaeresis
        
        # Fix combined characters
        'a\u0301': 'Ã¡', 'e\u0301': 'Ã©', 'i\u0301': 'Ã­', 'o\u0301': 'Ã³', 'u\u0301': 'Ãº',
        'A\u0301': 'Ã', 'E\u0301': 'Ã‰', 'I\u0301': 'Ã', 'O\u0301': 'Ã“', 'U\u0301': 'Ãš',
        'n\u0303': 'Ã±', 'N\u0303': 'Ã‘',
        'a\u0300': 'Ã ', 'e\u0300': 'Ã¨', 'i\u0300': 'Ã¬', 'o\u0300': 'Ã²', 'u\u0300': 'Ã¹',
        'a\u0308': 'Ã¤', 'e\u0308': 'Ã«', 'i\u0308': 'Ã¯', 'o\u0308': 'Ã¶', 'u\u0308': 'Ã¼',
    }
    
    # Apply MARC fixes first
    for wrong, correct in marc_fixes.items():
        text = text.replace(wrong, correct)
    
    # Common UTF-8 decoding issues - comprehensive list
    encoding_fixes = {
        # Spanish accented characters
        'aÃŒ': 'Ã¡', 'eÃŒ': 'Ã©', 'iÃŒ': 'Ã­', 'oÃŒ': 'Ã³', 'uÃŒ': 'Ãº',
        'AÃŒ': 'Ã', 'EÃŒ': 'Ã‰', 'IÃŒ': 'Ã', 'OÃŒ': 'Ã“', 'UÃŒ': 'Ãš',
        'Ã ÃŒ': 'Ã ', 'Ã¨ÃŒ': 'Ã¨', 'Ã¬ÃŒ': 'Ã¬', 'Ã²ÃŒ': 'Ã²', 'Ã¹ÃŒ': 'Ã¹',
        'Ã€ÃŒ': 'Ã€', 'ÃˆÃŒ': 'Ãˆ', 'ÃŒÃŒ': 'ÃŒ', 'Ã’ÃŒ': 'Ã’', 'Ã™ÃŒ': 'Ã™',
        
        # Ã‘ variations
        'nÃŒÆ’': 'Ã±', 'NÃŒÆ’': 'Ã‘', 'ÃƒÂ±': 'Ã±', 'ÃƒÃ‘': 'Ã‘', 'n~': 'Ã±', 'N~': 'Ã‘',
        
        # Common mojibake patterns
        'ÃƒÂ¡': 'Ã¡', 'ÃƒÂ©': 'Ã©', 'ÃƒÂ­': 'Ã­', 'ÃƒÂ³': 'Ã³', 'ÃƒÂº': 'Ãº',
        'Ãƒ ': 'Ã ', 'ÃƒÂ¨': 'Ã¨', 'ÃƒÂ¬': 'Ã¬', 'ÃƒÂ²': 'Ã²', 'ÃƒÂ¹': 'Ã¹',
        'Ãƒ': 'Ã', 'Ãƒâ€°': 'Ã‰', 'Ãƒ': 'Ã', 'Ãƒ"': 'Ã“', 'ÃƒÅ¡': 'Ãš',
        'Ãƒâ‚¬': 'Ã€', 'ÃƒË†': 'Ãˆ', 'ÃƒÅ’': 'ÃŒ', "Ãƒ'": 'Ã’', 'Ãƒâ„¢': 'Ã™',
        'ÃƒÂ¢': 'Ã¢', 'ÃƒÂª': 'Ãª', 'ÃƒÂ®': 'Ã®', 'ÃƒÂ´': 'Ã´', 'ÃƒÂ»': 'Ã»',
        'ÃƒÂ¤': 'Ã¤', 'ÃƒÂ«': 'Ã«', 'ÃƒÂ¯': 'Ã¯', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼',
        'ÃƒÂ§': 'Ã§', 'Ãƒâ€¡': 'Ã‡',
        
        # Double-encoded patterns
        'Ãƒ\x83Ã‚Â¡': 'Ã¡', 'Ãƒ\x83Ã‚Â©': 'Ã©', 'Ãƒ\x83Ã‚Â­': 'Ã­', 
        'Ãƒ\x83Ã‚Â³': 'Ã³', 'Ãƒ\x83Ã‚Âº': 'Ãº', 'Ãƒ\x83Ã‚Â±': 'Ã±',
        
        # Additional special characters
        'Ã¢â‚¬â„¢': "'", 'Ã¢â‚¬Å“': '"', 'Ã¢â‚¬': '"', 'Ã¢â‚¬"': 'â€“', 'Ã¢â‚¬"': 'â€”',
        'Ã‚Â°': 'Â°', 'Ã‚Â©': 'Â©', 'Ã‚Â®': 'Â®', 'Ã¢â€Â¢': 'â„¢',
        'â€¦': '...', 'â€¢': 'Â·',
        
        # Portuguese and French additions
        'ÃƒÂ£': 'Ã£', 'ÃƒÂµ': 'Ãµ', 'Ãƒâ€š': 'Ã‚', 'ÃƒÂª': 'Ãª', 'ÃƒÂ´': 'Ã´',
        'ÃƒÂ®': 'Ã®', 'ÃƒÂ»': 'Ã»', 'ÃƒÂ¯': 'Ã¯',
        
        # Catalan
        'Ãƒ ': 'Ã ', 'ÃƒÂ¨': 'Ã¨', 'ÃƒÂ²': 'Ã²', 'Ãƒ\xad': 'Ã­', 'Ãƒ\xba': 'Ãº',
    }
    
    # Apply all fixes
    for wrong, correct in encoding_fixes.items():
        text = text.replace(wrong, correct)
    
    # Try to detect and fix remaining encoding issues
    try:
        # Check if text might be double-encoded
        if 'Ãƒ' in text or 'Ã‚' in text:
            try:
                # Attempt to fix double UTF-8 encoding
                temp = text.encode('latin-1', errors='ignore').decode('utf-8', errors='ignore')
                # Only use if it reduces weird characters
                if temp.count('Ãƒ') < text.count('Ãƒ'):
                    text = temp
            except:
                pass
    except:
        pass
    
    # Normalize Unicode
    text = normalize_unicode(text)
    
    # Clean up MARC punctuation
    text = re.sub(r'\s*[/;:]\s*$', '', text)  # Remove trailing punctuation
    text = re.sub(r'\s+', ' ', text).strip()   # Normalize whitespace
    
    return text

def get_oauth_token():
    print("ğŸ” Getting OAuth token...")
    url = "https://oauth.oclc.org/token"
    auth = (WS_KEY, WS_SECRET)
    data = {"grant_type": "client_credentials", "scope": "wcapi"}
    resp = requests.post(url, auth=auth, data=data)
    resp.raise_for_status()
    return resp.json().get("access_token")

def fetch_metadata_json(oclc_number, token):
    url = f"https://americas.discovery.api.oclc.org/worldcat/search/v2/bibs/{oclc_number}"
    headers = {"Authorization": f"Bearer {token}"}
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    return None

def parse_basic_record(json_data, oclc_number):
    record = {
        "OCLC": str(oclc_number),
        "Title": "",
        "Creator": "",
        "Contributor": "",
        "Publisher": "",
        "Date": "",
        "URL": f"https://www.worldcat.org/oclc/{oclc_number}"
    }
    try:
        # Title
        titles = json_data.get("title", {}).get("mainTitles", [])
        if titles and isinstance(titles[0], dict):
            record["Title"] = fix_character_encoding(titles[0].get("text", "").split(" / ")[0].strip())

        # Creator (primary contributor)
        creators = json_data.get("contributor", {}).get("creators", [])
        if creators:
            first = creators[0].get("firstName", {}).get("text", "")
            last = creators[0].get("secondName", {}).get("text", "")
            record["Creator"] = fix_character_encoding(f"{first} {last}".strip())

        # Other Contributors
        others = json_data.get("contributor", {}).get("contributors", [])
        names = []
        for c in others:
            name = c.get("name", {}).get("text")
            if name:
                names.append(fix_character_encoding(name))
        record["Contributor"] = " ; ".join(names)

        # Publisher
        pubs = json_data.get("publishers", [])
        if pubs and isinstance(pubs[0], dict):
            record["Publisher"] = fix_character_encoding(pubs[0].get("name", ""))

        # Date
        record["Date"] = json_data.get("date", {}).get("publicationDate", "")

    except Exception as e:
        print(f"âŒ Error parsing OCLC {oclc_number}: {e}")

    return record

def write_to_csv(records, filepath):
    if not records:
        print("No data to write.")
        return
    keys = records[0].keys()
    with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=keys)
        writer.writeheader()
        writer.writerows(records)

# === MAIN RUN ===
if __name__ == "__main__":
    token = get_oauth_token()
    records = []
    for oclc in tqdm(OCLC_NUMBERS, desc="Fetching metadata"):
        data = fetch_metadata_json(oclc, token)
        record = parse_basic_record(data or {}, oclc)
        records.append(record)
    write_to_csv(records, OUTPUT_CSV_PATH)
    print(f"\nâœ… Done. Metadata saved to: {OUTPUT_CSV_PATH}")
